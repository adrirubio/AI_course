** RNN for time series prediction
   Most of the code in this notebook has been seen before in previous notebooks so will not be revised.
 
*** imports
    #+BEGIN_SRC python
    import torch
    import torch.nn as nn
    import numpy as np
    import matplotlib.pyplot as plt
    #+END_SRC

*** make the original data
    This generates the time series data set and then plots it.
    #+BEGIN_SRC python
    # make the original data
    N = 1000
    series = np.sin(0.1*np.arange(N)) #+ np.random.randn(N)*0.1
    
    # plot it
    plt.plot(series)
    plt.show() 
    #+END_SRC

*** build the dataset
    #+BEGIN_SRC python
    #+BEGIN_SRC python
    T = 10  # Number of time steps to look back
    X = []  # Input sequences
    Y = []  # Output values

    # Loop to create sequences (X) and corresponding labels (Y)
    for t in range(len(series) - T):
        x = series[t:t+T]  # Get a sequence of 'T' values from the series
        X.append(x)        # Append the sequence to the input list
        y = series[t+T]    # The next value after the sequence ends
        Y.append(y)        # Append this next value to the output list

    # Reshape data to fit the model input
    X = np.array(X).reshape(-1, T, 1)
    Y = np.array(Y).reshape(-1, 1)
    N = len(X)
    print("X.shape", X.shape, "Y.shape", Y.shape)
    #+END_SRC

*** define simple RNN
    
    #+BEGIN_SRC python
    ### Define simple RNN
    class SimpleRNN(nn.Module):
      def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):
        super(SimpleRNN, self).__init__()
        self.D = n_inputs
        self.M = n_hidden
        self.K = n_outputs
        self.L = n_rnnlayers

        # note: batch_first=True
        # applies the convention that our data will be of shape:
        # (num_samples, sequence_length, num_features)
        # rather than:
        # (sequence_length, num_samples, num_features)
        self.rnn = nn.RNN(
            input_size=self.D,
            hidden_size=self.M,
            num_layers=self.L,
            nonlinearity='relu',
            batch_first=True)
        self.fc = nn.Linear(self.M, self.K)
  
      def forward(self, X):
        # initial hidden states
        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)

        # get RNN unit output
        # out is of size (N, T, M)
        # 2nd return value is hidden states at each hidden layer
        # we don't need those now
        out, _ = self.rnn(X, h0)

        # we only want h(T) at the final time step
        # N x M -> N x K
        out = self.fc(out[:, -1, :])
        return out
    #+END_SRC

***** Explanation
****** Class Definition
   The SimpleRNN class inherits from nn.Module, which is a standard practice in PyTorch for defining neural network models.

****** Constructor (__init__)
   The constructor initializes the RNN with four parameters: n_inputs, n_hidden, n_rnnlayers, and n_outputs. These parameters represent the number of input features, hidden units, RNN layers, and output features, respectively. The class also defines an RNN layer and a fully connected (linear) layer.

****** RNN Layer
   The RNN layer is configured to use the 'relu' nonlinearity and assumes that the input tensor's first dimension represents the batch size (batch_first=True).

****** Fully Connected Layer
   This layer maps the final hidden state to the output space.

****** Forward Pass (forward)
   The forward method defines the forward pass of the network. It initializes the hidden state, computes the RNN output, and passes the last hidden state through the fully connected layer to produce the final output.

    
    #+BEGIN_SRC python
    # Instantiate the model
    model = SimpleRNN(n_inputs=1, n_hidden=15, n_rnnlayers=1, n_outputs=1)
    model.to(device)
    #+END_SRC 



*** loss and optimizer
   #+BEGIN_SRC python
   # Loss and optimizer
   criterion = nn.MSELoss()
   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
   #+END_SRC

*** prepare the data
   Here we split the data into parts and inputs and targets
   #+BEGIN_SRC python
   # Loss and optimizer
   criterion = nn.MSELoss()
   optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
   #+END_SRC
   
   Here we move the inputs and targets to the GPU
   #+BEGIN_SRC python
   # move data to GPU
   X_train, y_train = X_train.to(device), y_train.to(device)
   X_test, y_test = X_test.to(device), y_test.to(device)
   #+END_SRC

*** training
   Nothing new 
   #+BEGIN_SRC python
   # Training
   def full_gd(model,
               criterion,
               optimizer,
               X_train,
               y_train,
               X_test,
               y_test,
               epochs=1000):

     # Stuff to store
     train_losses = np.zeros(epochs)
     test_losses = np.zeros(epochs)

     for it in range(epochs):
       # zero the parameter gradients
       optimizer.zero_grad()

       # Forward pass
       outputs = model(X_train)
       loss = criterion(outputs, y_train)
      
       # Backward and optimize
       loss.backward()
       optimizer.step()

       # Save losses
       train_losses[it] = loss.item()

       # Test loss
       test_outputs = model(X_test)
       test_loss = criterion(test_outputs, y_test)
       test_losses[it] = test_loss.item()
      
       if (it + 1) % 5 == 0:
         print(f'Epoch {it+1}/{epochs}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}')
  
     return train_losses, test_losses
   #+END_SRC


*** start training
   #+BEGIN_SRC python
   train_losses, test_losses = full_gd(model,
                                       criterion,
                                       optimizer,
                                       X_train,
                                       y_train,
                                       X_test,
                                       y_test)
   #+END_SRC


   Example of epoch:
   #+BEGIN_SRC
   Epoch 5/1000, Train Loss: 0.3693, Test Loss: 0.3684
   Epoch 10/1000, Train Loss: 0.3410, Test Loss: 0.3394
   Epoch 15/1000, Train Loss: 0.3114, Test Loss: 0.3091
   Epoch 20/1000, Train Loss: 0.2806, Test Loss: 0.2775
   Epoch 25/1000, Train Loss: 0.2478, Test Loss: 0.2439
   Epoch 30/1000, Train Loss: 0.2148, Test Loss: 0.2111
   Epoch 35/1000, Train Loss: 0.1828, Test Loss: 0.1785
   Epoch 40/1000, Train Loss: 0.1496, Test Loss: 0.1450
   Epoch 45/1000, Train Loss: 0.1173, Test Loss: 0.1127
   Epoch 50/1000, Train Loss: 0.0882, Test Loss: 0.0843
   Epoch 55/1000, Train Loss: 0.0659, Test Loss: 0.0632
   Epoch 60/1000, Train Loss: 0.0529, Test Loss: 0.0517
   Epoch 65/1000, Train Loss: 0.0476, Test Loss: 0.0469
   Epoch 70/1000, Train Loss: 0.0436, Test Loss: 0.0427
   Epoch 75/1000, Train Loss: 0.0390, Test Loss: 0.0385
   Epoch 80/1000, Train Loss: 0.0362, Test Loss: 0.0362
   Epoch 85/1000, Train Loss: 0.0343, Test Loss: 0.0343
   Epoch 90/1000, Train Loss: 0.0322, Test Loss: 0.0322
   Epoch 95/1000, Train Loss: 0.0303, Test Loss: 0.0303
   Epoch 100/1000, Train Loss: 0.0288, Test Loss: 0.0287
   Epoch 105/1000, Train Loss: 0.0273, Test Loss: 0.0273
   Epoch 110/1000, Train Loss: 0.0260, Test Loss: 0.0259
   Epoch 115/1000, Train Loss: 0.0235, Test Loss: 0.0230
   Epoch 120/1000, Train Loss: 0.0192, Test Loss: 0.0185
   Epoch 125/1000, Train Loss: 0.0158, Test Loss: 0.0155
   Epoch 130/1000, Train Loss: 0.0130, Test Loss: 0.0124
   Epoch 135/1000, Train Loss: 0.0099, Test Loss: 0.0095
   Epoch 140/1000, Train Loss: 0.0070, Test Loss: 0.0065
   Epoch 145/1000, Train Loss: 0.0045, Test Loss: 0.0040
   Epoch 150/1000, Train Loss: 0.0024, Test Loss: 0.0022
   Epoch 155/1000, Train Loss: 0.0012, Test Loss: 0.0011
   Epoch 160/1000, Train Loss: 0.0007, Test Loss: 0.0006
   Epoch 165/1000, Train Loss: 0.0005, Test Loss: 0.0005
   Epoch 170/1000, Train Loss: 0.0004, Test Loss: 0.0004
   #+END_SRC

*** train and test loss
   Plot the loss
   #+BEGIN_SRC python
   # Plot the train loss and test loss per iteration
   plt.plot(train_losses, label='train loss')
   plt.plot(test_losses, label='test loss')
   plt.legend()
   plt.show()
   #+END_SRC

*** plot the predictions
   calculate the predictions
   #+BEGIN_SRC python
   T = 10  # Number of time steps to look back
   X = []  # Input sequences
   Y = []  # Output values

   # Creating the sequences of inputs (X) and the corresponding labels (Y)
   for t in range(len(series) - T):
       x = series[t:t+T]  # Extract a sequence of 'T' values from the series
       X.append(x)        # Append the sequence to the inputs list
       y = series[t+T]    # Get the next value in the series, post the sequence
       Y.append(y)        # Append this value to the labels list

   # Reshaping the data for model compatibility
   X = np.array(X).reshape(-1, T, 1)
   Y = np.array(Y).reshape(-1, 1)
   N = len(X)

   #+END_SRC

   plot the prediction
   #+BEGIN_SRC python
   plt.plot(validation_target, label='forecast target')
   plt.plot(validation_predictions, label='forecast prediction')
   plt.legend()
   #+END_SRC
   
   As a sidenote all the plots are in the images folder
   
*** Summary
   As of now you should know how to create a simple RNN   
