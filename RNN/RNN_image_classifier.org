** Image classifier
   In this model we will use the MNIST dataset but this time with and RNN.
   This model contains nothing new but is a good way to show our understanding.

*** Imports
#+BEGIN_SRC python
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
#+END_SRC

*** Load the data
    - Here we use the data loader
#+BEGIN_SRC python
train_dataset = torchvision.datasets.MNIST(
    root='.',
    train=True,
    transform=transforms.ToTensor(),
    download=True)
test_dataset = torchvision.datasets.MNIST(
    root='.',
    train=False,
    transform=transforms.ToTensor(),
    download=True)

batch_size = 128
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                           batch_size=batch_size,
                                           shuffle=True)

test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=batch_size,
                                          shuffle=False)
#+END_SRC

*** Building the model
    - We have already covered this code
#+BEGIN_SRC python
# Build the model
class RNN(nn.Module):
  def __init__(self, n_inputs, n_hidden, n_rnnlayers, n_outputs):
    super(RNN, self).__init__()
    self.D = n_inputs
    self.M = n_hidden
    self.K = n_outputs
    self.L = n_rnnlayers

    self.rnn = nn.LSTM(
        input_size=self.D,
        hidden_size=self.M,
        num_layers=self.L,
        batch_first=True)
    self.fc =nn.Linear(self.M, self.K)

  def forward(self, X):
    # initial hidden states
    h0 = torch.zeros(self.L, X.size(0), self.M).to(device)
    c0 = torch.zeros(self.L, X.size(0), self.M).to(device)

    # get RNN unit output
    out, _ = self.rnn(X, (h0, c0))

    # we only want h(T) at the final time step
    out = self.fc(out[:, -1, :])
    return out
#+END_SRC

#+BEGIN_SRC
model = RNN(28, 128, 2, 10)
#+END_SRC

*** Send the model to the GPU
#+BEGIN_SRC python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)
model.to(device)
#+END_SRC

*** Loss and optimizer
#+BEGIN_SRC python
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
#+END_SRC

*** Training
#+BEGIN_SRC  python
# Train the model
n_epochs = 10

# Stuff to store
train_losses = np.zeros(n_epochs)
test_losses = np.zeros(n_epochs)

for it in range(n_epochs):
  train_loss = []
  for inputs, targets in train_loader:
    # move data to GPU
    inputs, targets = inputs.to(device), targets.to(device)

    # reshape the input
    inputs = inputs.view(-1, 28, 28)

    # zero the parameter gradients
    optimizer.zero_grad()

    # Forward pass
    outputs = model(inputs)
    loss = criterion(outputs, targets)

    # Backward and optimize
    loss.backward()
    optimizer.step()

    train_loss.append(loss.item())

  # Get train loss and test loss
  train_loss = np.mean(train_loss) # a little misleading

  test_loss = []
  for inputs, targets in test_loader:
    inputs, targets = inputs.to(device), targets.to(device)
    inputs = inputs.view(-1, 28, 28)
    outputs = model(inputs)
    loss = criterion(outputs, targets)
    test_loss.append(loss.item())
  test_loss = np.mean(test_loss)

  # Save losses
  train_losses[it] = train_loss
  test_losses[it] = test_loss

  print(f'Epoch {it+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')
#+END_SRC

- Loss:
#+BEGIN_SRC
Epoch 1/10, Train Loss: 0.5545, Test Loss: 0.1612
Epoch 2/10, Train Loss: 0.1322, Test Loss: 0.0946
Epoch 3/10, Train Loss: 0.0836, Test Loss: 0.0753
Epoch 4/10, Train Loss: 0.0612, Test Loss: 0.0588
Epoch 5/10, Train Loss: 0.0478, Test Loss: 0.0557
Epoch 6/10, Train Loss: 0.0409, Test Loss: 0.0458
Epoch 7/10, Train Loss: 0.0349, Test Loss: 0.0533
Epoch 8/10, Train Loss: 0.0321, Test Loss: 0.0580
Epoch 9/10, Train Loss: 0.0303, Test Loss: 0.0501
Epoch 10/10, Train Loss: 0.0231, Test Loss: 0.0532
#+END_SRC

*** Plot the loss
#+BEGIN_SRC python
# Plot the train loss and test loss per iteration
plt.plot(train_losses, label='train loss')
plt.plot(test_losses, label='test loss')
plt.legend()
plt.show()
#+END_SRC

*** Accuracy
#+BEGIN_SRC python
n_correct = 0.
n_total = 0.
for inputs, targets in train_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)

  # reshape the input
  inputs = inputs.view(-1, 28, 28)

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)
  
  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

train_acc = n_correct / n_total


n_correct = 0.
n_total = 0.
for inputs, targets in test_loader:
  # move data to GPU
  inputs, targets = inputs.to(device), targets.to(device)
  
  # reshape the input
  inputs = inputs.view(-1, 28, 28)

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)
  
  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

test_acc = n_correct / n_total
print(f"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}")
#+END_SRC

    - Results:
#+BEGIN_SRC
Train acc: 0.9911, Test acc: 0.9850
#+END_SRC

As you can see these results are very good, much better than using a CNN.
