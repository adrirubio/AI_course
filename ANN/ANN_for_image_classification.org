* Full Walkthrough
  We start by importing these libraries:
  
  #+BEGIN_SRC python
  import torch
  import torch.nn as nn
  import torchvision
  import torchvision.transforms as transforms
  import numpy as np
  import matplotlib.pyplot as plt
  #+END_SRC
  
** The first step is to download the data
  
**** Train Data
  
  #+BEGIN_SRC python
  train_dataset = torchvision.datasets.MNIST(
    # specify where to download the data, '.' means the current directory
    root='.',
    # indicates that this is the train dataset
    train=True,
    # converts the images into PyTorch tensors
    transform=transforms.ToTensor(),
    download=True)

  # see the training data:
  train_dataset.data
  #+END_SRC

**** Test Data
 
  #+BEGIN_SRC python
  test_dataset = torchvision.datasets.MNIST(
    # specify where to download the data, '.' means the current directory
    root='.',
    # indicates that this is the test dataset by making train=False
    train=False,
    # converts the images into PyTorch tensors
    transform=transforms.ToTensor(),
    download=True)
  #+END_SRC  
    
  Note that nothing is downloaded because all the files were downloaded in the previous step
  
** Now it is time to build the model
   
   #+BEGIN_SRC python
   model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
   )
   #+END_SRC

  Lets break down what each part of the code does:
  - "nn.Sequential": This is a sequential container from PyTorch's neural network module (torch.nn)
  - "nn.Linear(784, 128)": The first argument (784) is the size of each input sample, and the second argument (128) is the size of each output sample.
  - "nn.ReLU()": ReLU is a commonly used activation function in neural networks, especially in hidden layers 
  - "nn.Linear(128, 10)": This takes the 128-dimensional output from the previous layer and transforms it into a 10-dimensional output

**** Using the GPU
  - There is a need to make use of the GPU we know that GPU's use are useful for speeding up deep learning
  
  #+BEGIN_SRC python
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(device)
  model.to(device)
  #+END_SRC
  
  - This code checks if a GPU is available otherwise it uses a CPU

**** Setting loss and optimizer
  - Here we set the loss and the optimizer
     
  #+BEGIN_SRC python
  # Loss and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters())
  #+END_SRC

**** Using a data loader
  - This is useful because it automatically generates batches in the training loop
  
  #+BEGIN_SRC python
  
  batch_size = 128
  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                             batch_size=batch_size, 
                                             shuffle=True)
  
  test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                            batch_size=batch_size, 
                                            shuffle=False)
  #+END_SRC

  - "batch_size = 128": This sets the batch size to 128
  - "train_loader = torch.utils.data.DataLoader(...)": This creates a data loader for the training dataset.
  - "test_loader = torch.utils.data.DataLoader(...)": This creates a data loader for the testing dataset.
  - "dataset=train_dataset": This specifies that the data loader should use "train_dataset" (which we defined earlier)
  - "dataset=test_dataset": This specifies that the data loader should use "test_dataset" (which we defined earlier)
  - "shuffle=True/False": This should be "True" on the training dataset but "False" on the testing dataset.

** Training the model
   Here we will train the model

   #+BEGIN_SRC python
   # Train the model
   n_epochs = 10

   # Stuff to store
   train_losses = np.zeros(n_epochs)
   test_losses = np.zeros(n_epochs)

   for it in range(n_epochs):
     train_loss = []
     for inputs, targets in train_loader:
       # move data to GPU
       inputs, targets = inputs.to(device), targets.to(device)

       # reshape the input
       inputs = inputs.view(-1, 784)

       # zero the parameter gradients
       optimizer.zero_grad()

       # Forward pass
       outputs = model(inputs)
       loss = criterion(outputs, targets)
      
       # Backward and optimize
       loss.backward()
       optimizer.step()

       train_loss.append(loss.item())

     # Get train loss and test loss
     train_loss = np.mean(train_loss) # a little misleading
  
     test_loss = []
     for inputs, targets in test_loader:
       inputs, targets = inputs.to(device), targets.to(device)
       inputs = inputs.view(-1, 784)
       outputs = model(inputs)
       loss = criterion(outputs, targets)
       test_loss.append(loss.item())
     test_loss = np.mean(test_loss)

     # Save losses
     train_losses[it] = train_loss
     test_losses[it] = test_loss
    
     print(f'Epoch {it+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')
  #+END_SRC   
   
**** Initialize Training Parameters
   - We start by setting the number of training epochs, which is the number of times the entire training dataset will be passed through the network.
   - Arrays for storing training and test losses are initialized. These arrays will hold the average loss for each epoch, allowing us to track how the model's performance improves over time.

**** Training Loop
   - The training process is executed over a specified number of epochs. An epoch consists of one full cycle through the entire training dataset.
   - In each epoch, we iterate over batches of data from the training dataset.

**** Batch Processing in Training Data
   - For each batch, we first extract the features (inputs) and labels (targets).
   - The data may be transferred to a device (like a GPU) for faster processing, although this detail is omitted in the simplified explanation.

**** Forward Pass
   - The model processes the inputs to make predictions. This step is called the forward pass.
   - We then calculate the loss, which measures how far the model's predictions are from the actual labels.

**** Backward Pass and Optimization
   - A backward pass is performed to calculate the gradients of the loss function with respect to the model's parameters.
   - The optimizer updates the model's parameters based on these gradients. This step is crucial for the model to learn from the data.

**** Tracking Training and Test Loss
   - After completing an epoch, we calculate and store the average training loss. This gives us feedback on how well the model is learning from the training data.
   - We also evaluate the model on a separate test dataset and record the test loss. This helps us assess how well the model generalizes to new, unseen data.

**** Summary
   - This training loop is the heart of training neural network models in machine learning. It involves forward and backward passes, optimization steps, and continuous evaluation.

   Now we sould get the train and test loss printed out:
   
   #+BEGIN_SRC
   Epoch 1/10, Train Loss: 0.4100, Test Loss: 0.2194
   Epoch 2/10, Train Loss: 0.1905, Test Loss: 0.1549
   Epoch 3/10, Train Loss: 0.1388, Test Loss: 0.1212
   Epoch 4/10, Train Loss: 0.1063, Test Loss: 0.1053
   Epoch 5/10, Train Loss: 0.0855, Test Loss: 0.0943
   Epoch 6/10, Train Loss: 0.0716, Test Loss: 0.0860
   Epoch 7/10, Train Loss: 0.0603, Test Loss: 0.0812
   Epoch 8/10, Train Loss: 0.0515, Test Loss: 0.0789
   Epoch 9/10, Train Loss: 0.0434, Test Loss: 0.0749
   Epoch 10/10, Train Loss: 0.0374, Test Loss: 0.0740 
   #+END_SRC

** Plotting the loss per iteration
   
   #+BEGIN_SRC python
   # Plot the train loss and test loss per iteration
   plt.plot(train_losses, label='train loss')
   plt.plot(test_losses, label='test loss')
   plt.legend()
   plt.show()
   #+END_SRC
   
   - Now you should end up with plot showing the train and test loss

** Calculating accuracy
   
   #+BEGIN_SRC python
   n_correct = 0.
   n_total = 0.
   for inputs, targets in train_loader:
     # move data to GPU
     inputs, targets = inputs.to(device), targets.to(device)

     # reshape the input
     inputs = inputs.view(-1, 784)

     # Forward pass
     outputs = model(inputs)

     # Get prediction
     # torch.max returns both max and argmax
     _, predictions = torch.max(outputs, 1)
  
     # update counts
     n_correct += (predictions == targets).sum().item()
     n_total += targets.shape[0]

   train_acc = n_correct / n_total


   n_correct = 0.
   n_total = 0.
   for inputs, targets in test_loader:
     # move data to GPU
     inputs, targets = inputs.to(device), targets.to(device)

     # reshape the input
     inputs = inputs.view(-1, 784)

     # Forward pass
     outputs = model(inputs)

     # Get prediction
     # torch.max returns both max and argmax
     _, predictions = torch.max(outputs, 1)
  
     # update counts
     n_correct += (predictions == targets).sum().item()
     n_total += targets.shape[0]

   test_acc = n_correct / n_total
   print(f"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}")
   #+END_SRC
   
**** Initializing Counters
   - The code begins by initializing counters for the correct predictions (`n_correct`) and the total number of samples (`n_total`). These are set to zero as floating-point numbers.

**** Processing the Training Data
   - The code iterates over the training data (`train_loader`), processing each batch of inputs and targets.
   - For each batch, the data is first moved to the appropriate device (GPU or CPU) for processing. This is important for computational efficiency, especially when using GPUs.
   - The input data is then reshaped to match the input requirements of the model. In the context of image data, this often involves flattening the image tensors.

**** Forward Pass and Predictions
   - A forward pass through the model is conducted to obtain the outputs (predictions).
   - The model's predictions are determined using `torch.max`, which provides the indices of the maximum values along the specified dimension, effectively returning the class with the highest predicted probability.

**** Updating Accuracy Counters
   - The code updates the count of correct predictions by comparing the model's predictions with the actual targets.
   - The total number of samples processed is also updated. This count is used to calculate the accuracy.

**** Calculating Training Accuracy
   - After processing all batches in the training dataset, the training accuracy is computed as the ratio of correct predictions to the total number of samples.

**** Repeating the Process for Test Data
   - The same process is repeated for the test dataset (`test_loader`). This step is crucial for evaluating how well the model generalizes to new, unseen data.

**** Final Accuracy Calculation and Output
   - Finally, the code calculates and prints the training and test accuracies. These metrics are fundamental for assessing the performance of the model in classification tasks, indicating how often the model makes correct predictions.

** Conclusion

   In summary, this document has provided a detailed explanation of how to create an image classificatin ANN model.





