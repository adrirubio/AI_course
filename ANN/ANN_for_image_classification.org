* Full Walkthrough
  We start by importing these libraries:
  
  #+BEGIN_SRC python
  import torch
  import torch.nn as nn
  import torchvision
  import torchvision.transforms as transforms
  import numpy as np
  import matplotlib.pyplot as plt
  #+END_SRC
  
** The first step is to download the data
  
  - Train Data:
  
  #+BEGIN_SRC python
  train_dataset = torchvision.datasets.MNIST(
    # specify where to download the data, '.' means the current directory
    root='.',
    # indicates that this is the train dataset
    train=True,
    # converts the images into PyTorch tensors
    transform=transforms.ToTensor(),
    download=True)

  # see the training data:
  train_dataset.data
  #+END_SRC

  - Test Data:

  #+BEGIN_SRC python
  test_dataset = torchvision.datasets.MNIST(
    # specify where to download the data, '.' means the current directory
    root='.',
    # indicates that this is the test dataset by making train=False
    train=False,
    # converts the images into PyTorch tensors
    transform=transforms.ToTensor(),
    download=True)
  #+END_SRC  
    
  Note that nothing is downloaded because all the files were downloaded in the previous step
  
** Now it is time to build the model
   
   #+BEGIN_SRC python
   model = nn.Sequential(
    nn.Linear(784, 128),
    nn.ReLU(),
    nn.Linear(128, 10)
   )
   #+END_SRC

  Lets break down what each part of the code does:
  - "nn.Sequential": This is a sequential container from PyTorch's neural network module (torch.nn)
  - "nn.Linear(784, 128)": The first argument (784) is the size of each input sample, and the second argument (128) is the size of each output sample.
  - "nn.ReLU()": ReLU is a commonly used activation function in neural networks, especially in hidden layers 
  - "nn.Linear(128, 10)": This takes the 128-dimensional output from the previous layer and transforms it into a 10-dimensional output

**** Using the GPU
  - There is a need to make use of the GPU we know that GPU's use are useful for speeding up deep learning
  
  #+BEGIN_SRC python
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
  print(device)
  model.to(device)
  #+END_SRC
  
  - This code checks if a GPU is available otherwise it uses a CPU

**** Setting loss and optimizer
  - Here we set the loss and the optimizer
     
  #+BEGIN_SRC python
  # Loss and optimizer
  criterion = nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters())
  #+END_SRC

**** Using a data loader
  - This is useful because it automatically generates batches in the training loop
  
  #+BEGIN_SRC python
  
  batch_size = 128
  train_loader = torch.utils.data.DataLoader(dataset=train_dataset, 
                                             batch_size=batch_size, 
                                             shuffle=True)
  
  test_loader = torch.utils.data.DataLoader(dataset=test_dataset, 
                                            batch_size=batch_size, 
                                            shuffle=False)
  #+END_SRC

  - "batch_size = 128": This sets the batch size to 128
  - "train_loader = torch.utils.data.DataLoader(...)": This creates a data loader for the training dataset.
  - "test_loader = torch.utils.data.DataLoader(...)": This creates a data loader for the testing dataset.
  - "dataset=train_dataset": This specifies that the data loader should use "train_dataset" (which we defined earlier)
  - "dataset=test_dataset": This specifies that the data loader should use "test_dataset" (which we defined earlier)
  - "shuffle=True/False": This should be "True" on the training dataset but "False" on the testing dataset.

** Training the model
   Here we will train the model

   #+BEGIN_SRC python
   # Train the model
   n_epochs = 10

   # Stuff to store
   train_losses = np.zeros(n_epochs)
   test_losses = np.zeros(n_epochs)

   for it in range(n_epochs):
     train_loss = []
     for inputs, targets in train_loader:
       # move data to GPU
       inputs, targets = inputs.to(device), targets.to(device)

       # reshape the input
       inputs = inputs.view(-1, 784)

       # zero the parameter gradients
       optimizer.zero_grad()

       # Forward pass
       outputs = model(inputs)
       loss = criterion(outputs, targets)
      
       # Backward and optimize
       loss.backward()
       optimizer.step()

       train_loss.append(loss.item())

     # Get train loss and test loss
     train_loss = np.mean(train_loss) # a little misleading
  
     test_loss = []
     for inputs, targets in test_loader:
       inputs, targets = inputs.to(device), targets.to(device)
       inputs = inputs.view(-1, 784)
       outputs = model(inputs)
       loss = criterion(outputs, targets)
       test_loss.append(loss.item())
     test_loss = np.mean(test_loss)

     # Save losses
     train_losses[it] = train_loss
     test_losses[it] = test_loss
    
     print(f'Epoch {it+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}')
  #+END_SRC   
   
**** Initialize Training Parameters
   - We start by setting the number of training epochs, which is the number of times the entire training dataset will be passed through the network.
   - Arrays for storing training and test losses are initialized. These arrays will hold the average loss for each epoch, allowing us to track how the model's performance improves over time.

**** Training Loop
   - The training process is executed over a specified number of epochs. An epoch consists of one full cycle through the entire training dataset.
   - In each epoch, we iterate over batches of data from the training dataset.

**** Batch Processing in Training Data
   - For each batch, we first extract the features (inputs) and labels (targets).
   - The data may be transferred to a device (like a GPU) for faster processing, although this detail is omitted in the simplified explanation.

**** Forward Pass
   - The model processes the inputs to make predictions. This step is called the forward pass.
   - We then calculate the loss, which measures how far the model's predictions are from the actual labels.

**** Backward Pass and Optimization
   - A backward pass is performed to calculate the gradients of the loss function with respect to the model's parameters.
   - The optimizer updates the model's parameters based on these gradients. This step is crucial for the model to learn from the data.

**** Tracking Training and Test Loss
   - After completing an epoch, we calculate and store the average training loss. This gives us feedback on how well the model is learning from the training data.
   - We also evaluate the model on a separate test dataset and record the test loss. This helps us assess how well the model generalizes to new, unseen data.

**** Summary
   - This training loop is the heart of training neural network models in machine learning. It involves forward and backward passes, optimization steps, and continuous evaluation.

   Now we sould get the train and test loss printed out:
   
   #+BEGIN_SRC
   Epoch 1/10, Train Loss: 0.4100, Test Loss: 0.2194
   Epoch 2/10, Train Loss: 0.1905, Test Loss: 0.1549
   Epoch 3/10, Train Loss: 0.1388, Test Loss: 0.1212
   Epoch 4/10, Train Loss: 0.1063, Test Loss: 0.1053
   Epoch 5/10, Train Loss: 0.0855, Test Loss: 0.0943
   Epoch 6/10, Train Loss: 0.0716, Test Loss: 0.0860
   Epoch 7/10, Train Loss: 0.0603, Test Loss: 0.0812
   Epoch 8/10, Train Loss: 0.0515, Test Loss: 0.0789
   Epoch 9/10, Train Loss: 0.0434, Test Loss: 0.0749
   Epoch 10/10, Train Loss: 0.0374, Test Loss: 0.0740 
   #+END_SRC
