*** Transfer Learning
- This is a simple transfer learning model

*** Imports
Very similar imports that we use in most models

#+BEGIN_SRC python
import torch
import torch.nn as nn
import torchvision
from torchvision import datasets, transforms, models
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import sys, os
from glob import glob
import imageio
#+END_SRC

*** downloading the data
Ok, in this model we are going to download the data from the terminal.
First go to your home directory and run this line of code:

#+BEGIN_SRC
wget -nc https://archive.org/download/food-5-k/Food-5K.zip
#+END_SRC

Next unzip the data

#+BEGIN_SRC
unzip -qq -o Food-5K.zip
#+END_SRC

Now if you run an "ls" you will see that there now is a new directory called "Food-5K"
After that you can run this "ls" to see the data

#+BEGIN_SRC
ls Food-5K/training
#+END_SRC

Which would print:

#+BEGIN_SRC
0_0.jpg     0_1387.jpg	0_422.jpg  0_809.jpg   1_1195.jpg  1_230.jpg  1_617.jpg
0_1000.jpg  0_1388.jpg	0_423.jpg  0_80.jpg    1_1196.jpg  1_231.jpg  1_618.jpg
0_1001.jpg  0_1389.jpg	0_424.jpg  0_810.jpg   1_1197.jpg  1_232.jpg  1_619.jpg
0_1002.jpg  0_138.jpg	0_425.jpg  0_811.jpg   1_1198.jpg  1_233.jpg  1_61.jpg
0_1003.jpg  0_1390.jpg	0_426.jpg  0_812.jpg   1_1199.jpg  1_234.jpg  1_620.jpg
0_1004.jpg  0_1391.jpg	0_427.jpg  0_813.jpg   1_119.jpg   1_235.jpg  1_621.jpg
0_1005.jpg  0_1392.jpg	0_428.jpg  0_814.jpg   1_11.jpg    1_236.jpg  1_622.jpg
0_1006.jpg  0_1393.jpg	0_429.jpg  0_815.jpg   1_1200.jpg  1_237.jpg  1_623.jpg
0_1007.jpg  0_1394.jpg	0_42.jpg   0_816.jpg   1_1201.jpg  1_238.jpg  1_624.jpg
0_1008.jpg  0_1395.jpg	0_430.jpg  0_817.jpg   1_1202.jpg  1_239.jpg  1_625.jpg
0_1009.jpg  0_1396.jpg	0_431.jpg  0_818.jpg   1_1203.jpg  1_23.jpg   1_626.jpg
0_100.jpg   0_1397.jpg	0_432.jpg  0_819.jpg   1_1204.jpg  1_240.jpg  1_627.jpg
0_1010.jpg  0_1398.jpg	0_433.jpg  0_81.jpg    1_1205.jpg  1_241.jpg  1_628.jpg
0_1011.jpg  0_1399.jpg	0_434.jpg  0_820.jpg   1_1206.jpg  1_242.jpg  1_629.jpg
0_1012.jpg  0_139.jpg	0_435.jpg  0_821.jpg   1_1207.jpg  1_243.jpg  1_62.jpg
0_1013.jpg  0_13.jpg	0_436.jpg  0_822.jpg   1_1208.jpg  1_244.jpg  1_630.jpg
0_1014.jpg  0_1400.jpg	0_437.jpg  0_823.jpg   1_1209.jpg  1_245.jpg  1_631.jpg
0_1015.jpg  0_1401.jpg	0_438.jpg  0_824.jpg   1_120.jpg   1_246.jpg  1_632.jpg
0_1016.jpg  0_1402.jpg	0_439.jpg  0_825.jpg   1_1210.jpg  1_247.jpg  1_633.jpg
0_1017.jpg  0_1403.jpg	0_43.jpg   0_826.jpg   1_1211.jpg  1_248.jpg  1_634.jpg
0_1018.jpg  0_1404.jpg	0_440.jpg  0_827.jpg   1_1212.jpg  1_249.jpg  1_635.jpg
0_1019.jpg  0_1405.jpg	0_441.jpg  0_828.jpg   1_1213.jpg  1_24.jpg   1_636.jpg
0_101.jpg   0_1406.jpg	0_442.jpg  0_829.jpg   1_1214.jpg  1_250.jpg  1_637.jpg
0_1020.jpg  0_1407.jpg	0_443.jpg  0_82.jpg    1_1215.jpg  1_251.jpg  1_638.jpg
0_1021.jpg  0_1408.jpg	0_444.jpg  0_830.jpg   1_1216.jpg  1_252.jpg  1_639.jpg
0_1022.jpg  0_1409.jpg	0_445.jpg  0_831.jpg   1_1217.jpg  1_253.jpg  1_63.jpg
0_1023.jpg  0_140.jpg	0_446.jpg  0_832.jpg   1_1218.jpg  1_254.jpg  1_640.jpg
#+END_SRC

Now lets run this on the terminal
#+BEGIN_SRC
mv Food-5K/* .
#+END_SRC

As you can see there are files that start with 0's and others that start with 1's
The files that start with 0's are nonfood images while the ones that start with 1's are food.

Lets plot some examples(Python):

#+BEGIN_SRC python
image_path = '/home/adrian/data/train/nonfood/0_951.jpg'

plt.imshow(imageio.imread(image_path))
plt.show()
#+END_SRC

#+BEGIN_SRC python
image_path = '/home/adrian/data/train/food/1_293.jpg'

plt.imshow(imageio.imread(image_path))
plt.show()
#+END_SRC

Next lets return to the terminal and run these lines of code
First lets create a directory and some sub directories and then add the files

#+BEGIN_SRC
mkdir data
#+END_SRC

#+BEGIN_SRC
data/train
#+END_SRC

#+BEGIN_SRC
data/test
#+END_SRC

#+BEGIN_SRC
data/train/nonfood
#+END_SRC

#+BEGIN_SRC
data/train/food
#+END_SRC

#+BEGIN_SRC
data/test/nonfood
#+END_SRC

#+BEGIN_SRC
data/test/food
#+END_SRC

Next lets add the files

#+BEGIN_SRC
mv training/0*.jpg data/train/nonfood
#+END_SRC

#+BEGIN_SRC
mv training/1*.jpg data/train/food
#+END_SRC

#+BEGIN_SRC
mv validation/0*.jpg data/test/nonfood
#+END_SRC

#+BEGIN_SRC
mv validation/1*.jpg data/test/food
#+END_SRC

Ok once this is finished we have downloaded the data and we can start working on the model.

*** transforming the images
In this code we apply some transformations/randomly change the images and therefore the model learns quicker and better.

#+BEGIN_SRC python
# Note: normalize mean and std are standardized for ImageNet
# https://github.com/pytorch/examples/blob/97304e232807082c2e7b54c597615dc0ad8f6173/imagenet/main.py#L197-L198
train_transform = transforms.Compose([
        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),
        transforms.RandomRotation(degrees=15),
        transforms.ColorJitter(),
        transforms.CenterCrop(size=224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

test_transform = transforms.Compose([
        transforms.Resize(size=256),
        transforms.CenterCrop(size=224),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
#+END_SRC

*** loading the data
Here we load the data "/home/adrian/data/train" and "/home/adrian/data/test" and applies the train and test transformations

#+BEGIN_SRC python
train_dataset = datasets.ImageFolder(
    'data/train',
    transform=train_transform
)
test_dataset = datasets.ImageFolder(
    'data/test',
    transform=test_transform
)
#+END_SRC

*** creating the data loader
We have done this step many times, we create a simple data loader for the train and test datasets

#+BEGIN_SRC python
batch_size = 128
train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=batch_size,
    shuffle=True
)
test_loader = torch.utils.data.DataLoader(
    test_dataset,
    batch_size=batch_size,
)
#+END_SRC

*** defining the model
- Here defining the model is different.

#+BEGIN_SRC python
# Define the model
model = models.vgg16(pretrained=True)

# Freeze VGG weights
for param in model.parameters():
  param.requires_grad = False
#+END_SRC

In this code we load the VGG16 model pre-trained on ImageNet.
Then, iterate over the model's parameters to set `requires_grad` to `False`, effectively freezing the weights so they are not updated during training.

*** replacing the classifier
Right now the model looks like this:

#+BEGIN_SRC python
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
#+END_SRC

But we need to replace the classifier so we can adapt it to our task.

First we see how many features the current classifier has.
So that our model has that many input features and two output features.

#+BEGIN_SRC python
# Retrieve the number of input features to the first layer of the original classifier
n_features = model.classifier[0].in_features

# Replace the original classifier with a new one for binary classification
model.classifier = nn.Linear(n_features, 2)
#+END_SRC

Now the model would look like this:

#+BEGIN_SRC python
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Linear(in_features=25088, out_features=2, bias=True)
)
#+END_SRC

*** use the GPU
No explaining needed here

#+BEGIN_SRC python
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
#+END_SRC

*** loss and optimizer

#+BEGIN_SRC python
# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())
#+END_SRC

*** training
Just like always we train the model

#+BEGIN_SRC python
def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):
  train_losses = np.zeros(epochs)
  test_losses = np.zeros(epochs)

  for it in range(epochs):
    t0 = datetime.now()
    train_loss = []
    for inputs, targets in train_loader:
      # move data to GPU

      # zero the parameter gradients
      optimizer.zero_grad()

      # Forward pass
      outputs = model(inputs)
      loss = criterion(outputs, targets)

      # Backward and optimize
      loss.backward()
      optimizer.step()

      train_loss.append(loss.item())

    # Get train loss and test loss
    train_loss = np.mean(train_loss) # a little misleading

    test_loss = []
    for inputs, targets in test_loader:
      outputs = model(inputs)
      loss = criterion(outputs, targets)
      test_loss.append(loss.item())
    test_loss = np.mean(test_loss)

    # Save losses
    train_losses[it] = train_loss
    test_losses[it] = test_loss

    dt = datetime.now() - t0
    print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \
      Test Loss: {test_loss:.4f}, Duration: {dt}')

  return train_losses, test_losses

train_losses, test_losses = batch_gd(
    model,
    criterion,
    optimizer,
    train_loader,
    test_loader,
    epochs=5,
)
#+END_SRC

The training would look something like this:

#+BEGIN_SRC
Epoch 1/5, Train Loss: 0.3514,       Test Loss: 0.0641, Duration: 0:00:40.585510
Epoch 2/5, Train Loss: 0.0625,       Test Loss: 0.0846, Duration: 0:00:40.623662
Epoch 3/5, Train Loss: 0.0463,       Test Loss: 0.0637, Duration: 0:00:40.593905
Epoch 4/5, Train Loss: 0.0289,       Test Loss: 0.0619, Duration: 0:00:40.574836
Epoch 5/5, Train Loss: 0.0162,       Test Loss: 0.0520, Duration: 0:00:40.545110
#+END_SRC

*** plotting the loss
Like always:

#+BEGIN_SRC python
# Plot the train loss and test loss per iteration
plt.plot(train_losses, label='train loss')
plt.plot(test_losses, label='test loss')
plt.legend()
plt.show()
#+END_SRC

*** accuracy
We see how well the model did.

#+BEGIN_SRC python
# Accuracy

n_correct = 0.
n_total = 0.
for inputs, targets in train_loader:

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

train_acc = n_correct / n_total


n_correct = 0.
n_total = 0.
for inputs, targets in test_loader:

  # Forward pass
  outputs = model(inputs)

  # Get prediction
  # torch.max returns both max and argmax
  _, predictions = torch.max(outputs, 1)

  # update counts
  n_correct += (predictions == targets).sum().item()
  n_total += targets.shape[0]

test_acc = n_correct / n_total
print(f"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}")
#+END_SRC

And as you can see the model did quite well:

#+BEGIN_SRC
Train acc: 0.9933, Test acc: 0.9870
#+END_SRC
