
** Full Walkthrough

*** Imports
    Same as usual imports except for datetime to calculate the time taken for each epoch
    #+BEGIN_SRC python
    import torch
    import torch.nn as nn
    import torchvision
    import torchvision.transforms as transforms
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    #+END_SRC

*** Loading the data
    Same as the ANN MNIST only changing the name

***** Training data
    #+BEGIN_SRC python
    train_dataset = torchvision.datasets.FashionMNIST(
    root='.',
    train=True,
    transform=transforms.ToTensor(),
    download=True)
    #+END_SRC

***** Test data
    #+BEGIN_SRC python
    test_dataset = torchvision.datasets.FashionMNIST(
       root='.',
       train=False,
       transform=transforms.ToTensor(),
       download=True)
    #+END_SRC

*** Defining the model
    #+BEGIN_SRC python
    # Define the model
    class CNN(nn.Module):
      def __init__(self, K):
        super(CNN, self).__init__()
	self.conv_layers = nn.Sequential(
          nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=2),
	  nn.ReLU(),
	  nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=2),
	  nn.ReLU(),
	  nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2),
	  nn.ReLU()
        )
        # http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html
	# "No zero padding, non-unit strides"
	# https://ptorch.org/docs/stable/nn.html
	self.dense_layers = nn.Sequential(
          nn.Dropout(0.2),
	  nn.Linear(128 * 2 * 2, 512),
	  nn.ReLU(),
	  nn.Dropout(0.2),
	  nn.Linear(512, K)
        )

      def forward(self, X):
        out = self.conv_layers(X)
	out = out.view(out.size(0), -1)
	out = self.dense_layers(out)
	return out
    #+END_SRC

**** Overview
   The CNN model is designed for image classification tasks and is structured in two main parts: convolutional layers for feature extraction and dense (fully connected) layers for classification.

***** Convolutional Layers
   The model begins with a series of convolutional layers, which are crucial for capturing spatial hierarchies and features in the input images.

***** Layer 1
    - Starts with a 2D convolutional layer.
    - It has 1 input channel, suitable for processing grayscale images.
    - Applies 32 filters (or kernels), each of size 3x3.
    - Uses a stride of 2, reducing the spatial dimensions of the output.

***** Layer 2
    - Another 2D convolutional layer.
    - Increases the number of filters to 64, allowing the network to learn more complex features.
    - Maintains a kernel size of 3x3 and a stride of 2.

***** Layer 3
    - The third convolutional layer.
    - Further increases the filters to 128.
    - Continues with the same kernel size and stride, progressively reducing the image dimensions and increasing the depth.

***** Activation Function
   - After each convolutional layer, a ReLU (Rectified Linear Unit) activation function is applied.
   - ReLU introduces non-linearity, allowing the model to learn complex patterns.

***** Transition to Dense Layers
   - The output from the convolutional layers is a set of high-level feature maps.
   - These feature maps are flattened into a one-dimensional vector before being fed into the dense layers.

***** Dense (Fully Connected) Layers
   - The flattened vector is processed through dense layers for the final classification.

***** Dropout Layer
    - A dropout layer is applied with a dropout rate of 0.2.
    - This helps prevent overfitting by randomly setting a fraction of the inputs to zero.

***** First Dense Layer
    - Transforms the flattened features into an intermediate representation of 512 dimensions.

***** Second Dropout Layer
    - Another dropout layer is used, also with a rate of 0.2, for additional regularization.

***** Final Dense Layer
    - The last dense layer maps the 512-dimensional vector to the number of output classes (denoted as K).
    - This layer's output can be used to determine the predicted class of the input image.

***** Conclusion
   The CNN model, through its architecture, effectively learns spatial hierarchies and complex patterns in the input data, making it suitable for image classification tasks. The combination of convolutional layers and dense layers, along with dropout for regularization, offers a balance between feature extraction and computational efficiency.


***** Instantiate the model
      #+BEGIN_SRC python
      # Instantiate the model
      model = CNN(K)
      #+END_SRC

*** Move data to GPU
    #+BEGIN_SRC python
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    print(device)
    model.to(device)
    #+END_SRC

*** Loss and optimizer
    Nothing new
    #+BEGIN_SRC python
    # Loss and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters())
    #+END_SRC

*** Data loader
    #+BEGIN_SRC python
    # Data loader
    # Useful because it automatically generates batches in the training loop
    # and takes care of shuffling

    batch_size = 128
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=True)

    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=batch_size,
                                              shuffle=False)
    #+END_SRC

*** Training the model
    The same as with the ANN model except for datetime calculation here and there
    #+BEGIN_SRC python
      # A function to encapsulate the training loop
      def batch_gd(model, criterion, optimizer, train_loader, test_loader, epochs):
	train_losses = np.zeros(epochs)
	test_losses = np.zeros(epochs)

	for it in range(epochs):
	  model.train()
	  t0 = datetime.now()
	  train_loss = []
	  for inputs, targets in train_loader:
	    # move data to GPU
	    inputs, targets = inputs.to(device), targets.to(device)

	    # zero the parameter gradients
	    optimizer.zero_grad()

	    # Forward pass
	    outputs = model(inputs)
	    loss = criterion(outputs, targets)

	    # Backward and optimize
	    loss.backward()
	    optimizer.step()

	    train_loss.append(loss.item())

	  # Get train loss and test loss
	  train_loss = np.mean(train_loss) # a little misleading

	  model.eval()
	  test_loss = []
	  for inputs, targets in test_loader:
	    inputs, targets = inputs.to(device), targets.to(device)
	    outputs = model(inputs)
	    loss = criterion(outputs, targets)
	    test_loss.append(loss.item())
	    test_loss = np.mean(test_loss)

	  # Save losses
	  train_losses[it] = train_loss
	  test_losses[it] = test_loss

	  dt = datetime.now() - t0
	  print(f'Epoch {it+1}/{epochs}, Train Loss: {train_loss:.4f}, \
	    Test Loss: {test_loss:.4f}, Duration: {dt}')

	return train_losses, test_losses

      train_losses, test_losses = batch_gd(
	model, criterion, optimizer, train_loader, test_loader, epochs=15)
    #+END_SRC

***** Output from training
      #+BEGIN_SRC
      Epoch 1/15, Train Loss: 0.6592,       Test Loss: 0.4777, Duration: 0:00:06.645706
      Epoch 2/15, Train Loss: 0.4356,       Test Loss: 0.4024, Duration: 0:00:06.558660
      Epoch 3/15, Train Loss: 0.3769,       Test Loss: 0.3742, Duration: 0:00:06.649391
      Epoch 4/15, Train Loss: 0.3434,       Test Loss: 0.3386, Duration: 0:00:06.523967
      Epoch 5/15, Train Loss: 0.3153,       Test Loss: 0.3307, Duration: 0:00:06.601004
      Epoch 6/15, Train Loss: 0.2930,       Test Loss: 0.3230, Duration: 0:00:06.542776
      Epoch 7/15, Train Loss: 0.2726,       Test Loss: 0.3002, Duration: 0:00:06.541138
      Epoch 8/15, Train Loss: 0.2557,       Test Loss: 0.2898, Duration: 0:00:06.527418
      Epoch 9/15, Train Loss: 0.2457,       Test Loss: 0.2863, Duration: 0:00:06.535334
      Epoch 10/15, Train Loss: 0.2306,       Test Loss: 0.2925, Duration: 0:00:06.607582
      Epoch 11/15, Train Loss: 0.2191,       Test Loss: 0.2857, Duration: 0:00:06.593547
      Epoch 12/15, Train Loss: 0.2024,       Test Loss: 0.2981, Duration: 0:00:06.591980
      Epoch 13/15, Train Loss: 0.1940,       Test Loss: 0.2992, Duration: 0:00:06.571185
      Epoch 14/15, Train Loss: 0.1832,       Test Loss: 0.2810, Duration: 0:00:06.572984
      Epoch 15/15, Train Loss: 0.1737,       Test Loss: 0.2900, Duration: 0:00:06.572188
      #+END_SRC


*** Plot the loss
    Same as with ANN
    #+BEGIN_SRC python
    # Plot the train loss and test loss per iteration
    plt.plot(train_losses, label='train loss')
    plt.plot(test_losses, label='test loss')
    plt.legend()
    plt.show()
    #+END_SRC

*** Calculate Accuracy
    Same as with ANN
    #+BEGIN_SRC python
      # Accuracy

      model.eval()
      n_correct = 0.
      n_total = 0.
      for inputs, targets in train_loader:
	# move data to GPU
	inputs, targets = inputs.to(device), targets.to(device)

	# Forward pass
	outputs = model(inputs)

	# Get prediction
	# torch.max returns both max and argmax
	_, predictions = torch.max(outputs, 1)

	# update counts
	n_correct += (predictions == targets).sum().item()
	n_total += targets.shape[0]

      train_acc = n_correct / n_total


      n_correct = 0.
      n_total = 0.
      for inputs, targets in test_loader:
	# move data to GPU
	inputs, targets = inputs.to(device), targets.to(device)

	# Forward pass
	outputs = model(inputs)

	# Get prediction
	# torch.max returns both max and argmax
	_, predictions = torch.max(outputs, 1)

	# update counts
	n_correct += (predictions == targets).sum().item()
	n_total += targets.shape[0]

      test_acc = n_correct / n_total
      print(f"Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}")
    #+END_SRC

***** Output from accuracy
      #+BEGIN_SRC
      Train acc: 0.9513, Test acc: 0.9000
      #+END_SRC

*** Conclusion
