** NLP for text classification
- This notebook is a custom NLP and some code will be new

*** imports
#+BEGIN_SRC python
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import requests
import io
#+END_SRC

- Some of the imports are new

*** load the data
Here we load the data from a url:
#+BEGIN_SRC python
url = 'https://lazyprogrammer.me/course_files/spam.csv'
response = requests.get(url)
response.raise_for_status()  # Raise an error if download fails

df = pd.read_csv(io.StringIO(response.content.decode('latin-1')), encoding='latin-1')
#+END_SRC

*** prepare the file
Using pandas we can prepare the file to later create the dataset
#+BEGIN_SRC python
# drop unnecessary columns
df = df.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)

# remame columns to something better
df.columns = ['labels', 'data']

# create binary labels
df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})
#+END_SRC

*** map an integer to each token
- In this code we divide the file into df_train and df_test
#+BEGIN_SRC python
df_train, df_test = train_test_split(df, test_size=0.33)
df_train.shape, df_test.shape
#+END_SRC

This would print

#+BEGIN_SRC
((3733, 3), (1839, 3))
#+END_SRC

- In this next code padding is 0 so we start mapping integers to tokens from 1 onward
#+BEGIN_SRC python
# 0 = padding
idx = 1 # starts at 1
word2idx = {'<PAD>': 0}
#+END_SRC

- In this code we looped through every item and gave it an integer and added it to a dictionary

#+BEGIN_SRC python
# you could always use gensim or spacy for tokenization,
# but let's keep it simple!
for i, row in df_train.iterrows():
  tokens = row['data'].lower().split() # simple tokenization
  for token in tokens:
    if token not in word2idx:
      word2idx[token] = idx
      idx += 1
#+END_SRC

Now word2idx has is dictionary of words all of them mapped to an integer:

#+BEGIN_SRC
{'<PAD>': 0,
 'many': 1,
 'more': 2,
 'happy': 3,
 'returns': 4,
 'of': 5,
 'the': 6,
 'day.': 7,
 'i': 8,
 'wish': 9,
 'you': 10,
 'birthday.': 11,
 'hmmm': 12,
 '...': 13,
 'and': 14,
 'imagine': 15,
 'after': 16,
 "you've": 17,
 'come': 18,
 'home': 19,
 'from': 20,
 'that': 21,
 'having': 22,
 'to': 23,
 'rub': 24,
 'my': 25,
 'feet,': 26,
 'make': 27,
 'me': 28,
 'dinner': 29
 ...
#+END_SRC

#+BEGIN_SRC python
len(word2idx)
#+END_SRC

- This would print

#+BEGIN_SRC
10445
#+END_SRC

*** Create train and test data
- In this code we convert data into word indices

#+BEGIN_SRC python
# convert data into word indices
# note: could have done this on the fly earlier
train_sentences_as_int = []
for i, row in df_train.iterrows():
  tokens = row['data'].lower().split()
  sentence_as_int = [word2idx[token] for token in tokens] # finds the integer of the token
  train_sentences_as_int.append(sentence_as_int)
#+END_SRC

- Now we do the same but with the test data

#+BEGIN_SRC python
test_sentences_as_int = []
for i, row in df_test.iterrows():
  tokens = row['data'].lower().split()
  sentence_as_int = [word2idx[token] for token in tokens if token in word2idx]
  test_sentences_as_int.append(sentence_as_int)
#+END_SRC

As expected there are more train sentences than test sentences

#+BEGIN_SRC python
len(train_sentences_as_int), len(test_sentences_as_int)
#+END_SRC

Result:

#+BEGIN_SRC
(3733, 1839)
#+END_SRC














