** NLP for text classification
- This notebook is a custom NLP and some code will be new

*** imports
#+BEGIN_SRC python
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import requests
import io
#+END_SRC

- Some of the imports are new

*** load the data
Here we load the data from a url:
#+BEGIN_SRC python
url = 'https://lazyprogrammer.me/course_files/spam.csv'
response = requests.get(url)
response.raise_for_status()  # Raise an error if download fails

df = pd.read_csv(io.StringIO(response.content.decode('latin-1')), encoding='latin-1')
#+END_SRC

*** prepare the file
Using pandas we can prepare the file to later create the dataset
#+BEGIN_SRC python
# drop unnecessary columns
df = df.drop(["Unnamed: 2", "Unnamed: 3", "Unnamed: 4"], axis=1)

# remame columns to something better
df.columns = ['labels', 'data']

# create binary labels
df['b_labels'] = df['labels'].map({'ham': 0, 'spam': 1})
#+END_SRC

*** map an integer to each token
- In this code we divide the file into df_train and df_test
#+BEGIN_SRC python
df_train, df_test = train_test_split(df, test_size=0.33)
df_train.shape, df_test.shape
#+END_SRC

This would print

#+BEGIN_SRC
((3733, 3), (1839, 3))
#+END_SRC

- In this next code padding is 0 so we start mapping integers to tokens from 1 onward
#+BEGIN_SRC python
# 0 = padding
idx = 1 # starts at 1
word2idx = {'<PAD>': 0}
#+END_SRC

- In this code we looped through every item and gave it an integer and added it to a dictionary

#+BEGIN_SRC python
# you could always use gensim or spacy for tokenization,
# but let's keep it simple!
for i, row in df_train.iterrows():
  tokens = row['data'].lower().split() # simple tokenization
  for token in tokens:
    if token not in word2idx:
      word2idx[token] = idx
      idx += 1
#+END_SRC

Now word2idx has is dictionary of words all of them mapped to an integer:

#+BEGIN_SRC
{'<PAD>': 0,
 'many': 1,
 'more': 2,
 'happy': 3,
 'returns': 4,
 'of': 5,
 'the': 6,
 'day.': 7,
 'i': 8,
 'wish': 9,
 'you': 10,
 'birthday.': 11,
 'hmmm': 12,
 '...': 13,
 'and': 14,
 'imagine': 15,
 'after': 16,
 "you've": 17,
 'come': 18,
 'home': 19,
 'from': 20,
 'that': 21,
 'having': 22,
 'to': 23,
 'rub': 24,
 'my': 25,
 'feet,': 26,
 'make': 27,
 'me': 28,
 'dinner': 29
 ...
#+END_SRC

#+BEGIN_SRC python
len(word2idx)
#+END_SRC

- This would print

#+BEGIN_SRC
10445
#+END_SRC

*** create train and test data
- In this code we convert data into word indices

#+BEGIN_SRC python
# convert data into word indices
# note: could have done this on the fly earlier
train_sentences_as_int = []
for i, row in df_train.iterrows():
  tokens = row['data'].lower().split()
  sentence_as_int = [word2idx[token] for token in tokens] # finds the integer of the token
  train_sentences_as_int.append(sentence_as_int)
#+END_SRC

- Now we do the same but with the test data

#+BEGIN_SRC python
test_sentences_as_int = []
for i, row in df_test.iterrows():
  tokens = row['data'].lower().split()
  sentence_as_int = [word2idx[token] for token in tokens if token in word2idx]
  test_sentences_as_int.append(sentence_as_int)
#+END_SRC

As expected there are more train sentences than test sentences

#+BEGIN_SRC python
len(train_sentences_as_int), len(test_sentences_as_int)
#+END_SRC

Result:

#+BEGIN_SRC
(3733, 1839)
#+END_SRC


*** creating the data generator
#+BEGIN_SRC python
  def data_generator(X, y, batch_size=32):
      X, y = shuffle(X, y)
      n_batches = int(np.ceil(len(y) / batch_size))
      for i in range(n_batches):
          end = min((i + 1) * batch_size, len(y))

          X_batch = X[i * batch_size:end]
          y_batch = y[i * batch_size:end]

          # pad X_batch to be N x T
          max_len = np.max([len(x) for x in X_batch])
          for j in range(len(X_batch)):
              x = X_batch[j]
              pad = [0] * (max_len - len(x))
              X_batch[j] = pad + x

          # convert to tensor
          X_batch = torch.from_numpy(np.array(X_batch)).long()
          y_batch = torch.from_numpy(np.array(y_batch)).long()

          yield X_batch, y_batch
#+END_SRC


**** Explanation
Now let's break down the function step by step:

**** Input Parameters:
   - `X`: Input data, typically a list or array of input sequences.
   - `y`: Output labels corresponding to the input data.
   - `batch_size`: Number of samples per batch. Default is set to 32.

**** Data Shuffling:
   The input data `X` and labels `y` are shuffled to randomize the order of samples before batching.

**** Batching:
   - The total number of batches (`n_batches`) is calculated based on the length of the output labels `y` and the specified `batch_size`.
   - The function iterates over each batch index `i`.
   - For each batch, it selects a subset of input data and labels (`X_batch` and `y_batch`) based on the batch index `i`.
   - If the data size is not perfectly divisible by the batch size, the last batch may contain fewer samples.

**** Padding:
   - The input sequences in `X_batch` are padded to ensure uniform length within each batch.
   - The maximum length of sequences in the current batch (`max_len`) is calculated.
   - Each input sequence in `X_batch` is padded with zeros to match the maximum length.

**** Data Conversion:
   - The padded input sequences (`X_batch`) and corresponding labels (`y_batch`) are converted into PyTorch tensors.
   - The input sequences are converted to tensors of type `long`.

**** Yielding Batches:
   - The function yields each batch of input-output pairs (`X_batch`, `y_batch`) using the `yield` keyword.
   - This allows the function to be used in a loop for generating batches during the training process.

